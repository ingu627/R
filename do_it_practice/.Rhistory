loc.begin<-as.vector(gregexpr("ing", mysentence)[[1]])
mysentences<-unlist(rWikiSent)
mysentences
library(tm)
install.package('tm')
install.packages('tm')
library(tm)
s
1+1
library(tm)
1+!
library(stringr)
mytext
mytext <- c("She is an actor","She is the actor")
mytext
# install.packages('tm')
library(tm)
mytext
mystemmer.func <- function(mytextobj){
mytext<-str_replace_all(mytextobj, '(i|I)s |was |are |am |were ', 'be ')
mytext
}
mystemmer.func(mytext)
mytext <- c("I am a boy. You are a boy. The person might be a boy. Is Jane a boy?")
mystemmer.func(mytext)
mytext <- "The United States comprises fifty states. In the United States, each state has its own laws. However, federal law overrides state law in the United States."
mytext
unlist(str_extract_all(mytext, boundary('word')))
myword<- unlist(str_extract_all(mytext, boundary('word')))
table(myword)
str_replace_all(mytext, 'United States', 'United_States')
myword <- unlist(str_replace_all(mytext, 'United States', 'United_States') #고유명사 처리)
myword
myword <- unlist(str_replace_all(mytext, 'United States', 'United_States')) #고유명사 처리
myword
mytext.2gram <-str_replace_all(mytext, 'United States', 'United_States') #고유명사 처리
mytext.2gram
str_extract_all(mytext.2gram, boundary('word'))
unlist(str_extract_all(mytext.2gram, boundary('word')))
myword2<- unlist(str_extract_all(mytext.2gram, boundary('word')))
table(myword2)
length(table(myword2)) #단어 종류
sum(table(myword2))
mytext.3gram <-str_replace_all(mytext, '(t|T)he United States', 'United_States') #고유명사 처리
myword3<- unlist(str_extract_all(mytext.3gram, boundary('word')))
table(myword3)
length(table(myword3)) #단어 종류
sum(table(myword3)) #단어 전체 개수
my.text.location <- '../papers/papers'
my.text.location
mypaper<-VCorpus(DirSource(my.text.location))
mypaper
mypaper
summary(mypaper)
mypaper[[2]]
meta(mypaper[[2]]
mypaper[[2]]
mypaper[[2]]$meta
myfunc <- function(x){
print(x$content)
}
# print(x$content)
str_extract_all(x$content, "[[:alnum:]]{1,}[[:punct:]]{1}")
# print(x$content)
str_extract_all(x$content, "[[:alnum:]]{1,}[[:punct:]]{1}[[:alnum:]]{1,}")
meta(mypaper[[2]], tag='author')
mypaper[[2]]$meta
mypuncts<-lapply(mypaper, myfunc)
table(unlist(mypuncts))
mypuncts<-lapply(mypaper, myfunc)
myfunc <- function(x){
# print(x$content)
str_extract_all(x$content, "[[:alnum:]]{1,}[[:punct:]]{1}[[:alnum:]]{1,}")
}
mypuncts<-lapply(mypaper, myfunc)
table(unlist(mypuncts))
# 수치가 포함된 자료 추출
myfunc <- function(x){
str_extract_all(x$content, "[[:digit]]+")
}
mypuncts<-lapply(mypaper, myfunc)
table(unlist(mypuncts))
mypaper<- meta(mypaper[[2]], tag='author')
# 수치가 포함된 자료 추출
myfunc <- function(x){
str_extract_all(x$content, "[[:digit]]+")
}
mypuncts<-lapply(mypaper, myfunc)
table(unlist(mypuncts))
summary(mypaper)
my.text.location <- '../papers/papers'
mypaper<-VCorpus(DirSource(my.text.location))
mypaper
# 수치가 포함된 자료 추출
myfunc <- function(x){
str_extract_all(x$content, "[[:digit]]+")
}
mypuncts<-lapply(mypaper, myfunc)
table(unlist(mypuncts))
# 수치가 포함된 자료 추출
myfunc <- function(x){
str_extract_all(x$content, "[[:alpha:]]?[[:digit]]+[[:alpha:]]?")
}
mypuncts<-lapply(mypaper, myfunc)
table(unlist(mypuncts))
# tm_map함수
# tm.map(코펏, 처리작업)
tm_map()
# tm_map함수
# tm.map(코펏, 처리작업)
res<-tm_map(mypapper, content_transformertwolower)
# tm_map함수
# tm.map(코펏, 처리작업)
res<-tm_map(mypaper, content_transformertwolower)
# tm_map함수
# tm.map(코펏, 처리작업)
res<-tm_map(mypaper, content_transformertwolower)
# tm_map함수
# tm.map(코펏, 처리작업)
res<-tm_map(mypaper, content_transformer(twolower))
# tm_map함수
# tm.map(코펏, 처리작업)
res<-tm_map(mypaper, content_transformer(tolower))
res[[2]]$content
mypaper[[2]]$content
res[[2]]$content
#숫자 제거
mycorpus <- tm_map(mpaper, removeNumbers)
mycorpus[[2]]$content
#숫자 제거
mycorpus <- tm_map(mpaper, removeNumbers)
#숫자 제거
mycorpus <- tm_map(mypaper, removeNumbers)
mycorpus[[2]]$content
# 공백 2개 이상 -> 공백 1개
mycorpus <- tm_map(mypaper, stripWhitespace)
mycorpus[[2]]$content
#숫자 제거
mycorpus <- tm_map(mypaper, removeNumbers)
mycorpus[[2]]$content
# 공백 2개 이상 -> 공백 1개
mycorpus <- tm_map(mycorpus, stripWhitespace)
mycorpus[[2]]$content
# 소문자로 변환
mycorpus <- tm_map(mycorpus, content_transformer(tolower))
mycorpus[[2]]$content
# 소문자로 변환
mycorpus <- tm_map(mycorpus, content_transformer(tolower))
mycorpus[[2]]$content
# 불용어 제거
stopwords('SMART')
# 불용어 제거(mycorpus에서 stopwords('SMART') 모두 제거)
mycorpus <-tm_map(mycorpus, removeWords, words=stopwords('SMART'))
read.csv('sms_spam_ansi.txt')
read.csv('f:/data/sms_spam_ansi.txt')
# mypaper 말붕치는 리스트 형식
mypaper[[2]]
mypaper[[2]]$content
mypaper[[2]]$meta
meta(mypaper[[2]], tag='author')<- 'Kim'
mypaper[[2]]$meta
myfunc <- function(x){
# print(x$content)
str_extract_all(x$content, "[[:alnum:]]{1,}[[:punct:]]{1}[[:alnum:]]{1,}")
}
sms_raw<-read.csv('f:/data/sms_spam_ansi.txt')
str(sms_raw)
# 타겟변수는 범주형으로 되어야함
sms_raw$type
# 타겟변수는 범주형으로 되어야함
factor(sms_raw$type)
# 타겟변수는 범주형으로 되어야함
sms_raw$type <- factor(sms_raw$type)
table(sms_raw) # 데이터 빈도 확인인
table(sms_raw$type) # 데이터 빈도 확인
VectorSource(sms_raw$text)
Vorpus(VectorSource(sms_raw$text))
Vcorpus(VectorSource(sms_raw$text))
VCorpus(VectorSource(sms_raw$text))
smscorpus<-VCorpus(VectorSource(sms_raw$text))
smscorpus
smscorpus[[1]]$content
smscorpus[[2]]$content
smscorpus[[5559]]$content
print(smscorpus)
inspect(smscorpus)
#내용을 확인하려면 -> smscorpus[[1]]$content
inspect(smscorpus[1:10])
smscorpus[[1]]
as.character(smscorpus[[1]])
lapply(smscorpus[1:10, as.character])
lapply(smscorpus[1:10], as.character)
#전처리 작업
smscorpus_clean <-tm_map(smscorpus, content_transformer(tolower))
lapply(smscorpus_clean[1:10], as.character)
#숫자 제거
smscorpus_clean<-tm_map(smscorpus_clean, removeNumbers)
#불용어 제거
smscorpus_clean<-tm_map(smscorpus_clean, removeWords, stopwords())
#특수문자 제거
smscorpus_clean<-tm_map(smscorpus_clean, removePunctuation)
install.packages('SnowballC')
#install.packages('SnowballC')
library(SnowballC)
wordStem(c('learn', 'learned', 'learning', 'learns'))
t<-c('learn', 'learned', 'learning', 'learns')
stemDocument(t)
smscorpus_clean<-tm_map(smscorpus_clean, stemDocument)
smscorpus_clean<-tm_map(smscorpus_clean, stipWhitespace)
smscorpus_clean<-tm_map(smscorpus_clean, stripWhitespace)
lapply(smscorpus_clean[1:3], as.character)
laaply(smscorpus[1:3], as.character)
lapply(smscorpus[1:3], as.character)
DocumentTermMatrix(smscorpus_clean)
smsdtm<-DocumentTermMatrix(smscorpus_clean) #DTM
smsdtm
smsdtm_train<-smsdtm[1:4169,]
smsdtm_train<-smsdtm[4170:5559,]
smsdtm_train
smsdtm_train<-smsdtm[1:4169,]
smsdtm_test<-smsdtm[4170:5559,]
smsdtm_train
smsdtm_test
sms_raw[1:4169,]$type
sms_train_labels<- sms_raw[1:4169,]$type
sms_test_labels<- sms_raw[4170:5559,]$type
sms_train_labels
sms_test_labels
table(sms_train_labels)
prop.table(sms_train_labels)
prop.table(table(sms_train_labels))
prop.table(table(sms_test_labels))
install.packages('wordcloud')
# install.packages('wordcloud')
library(wordcloud)
wordcloud(smscorpus_clean)
wordcloud(smscorpus_clean, min.freq = 50)
#어떤 단어가 많이 등장?
#스팸
#햄
sms_raw[sms_raw$type=='spam']
#어떤 단어가 많이 등장?
#스팸
#햄
sms_raw[sms_raw$type=='spam',]
sms_raw %>% filter(type == 'spam') %>% select(text)
spam<-subset(sms_raw, type='spam')
ham<-subset(sms_raw, type='ham')
wordcloud(spam$text, max.words=40)
wordcloud(ham$text, max.words=40)
removeSparseTerms(smsdtm_train, 0.999)
smsdtm_freq_train<-removeSparseTerms(smsdtm_train, 0.999)
smsdtm_freq_train
library(stringr)
mytext<-c("software environment","software  environment","software\tenvironment")
mytext
str_split(mytext,' ') #결과는 list로 나옴
# 각 벡터의 길이(단어 개수) (결과 벡터로 출력)
sapply(str_split(mytext, ' '), length)
# 리스트로 출력
lapply(str_split(mytext, ' '), length)
# 문자 개수
sapply(str_split(mytext, ' '), str_length)
mytext
mytext.nowhitespace=str_replace_all(mytext, "[[:space:]]{1,}"," ")
sapply(str_split(mytext.nowhitespace, ' '), length)
sapply(str_split(mytext.nowhitespace, ' '), str_length)
# 대소문자 통일
mytext <- "The 45th President of the United States, Donald Trump, states that he knows how to play trump with the former president"
mytext
myword<-unlist(str_extract_all(mytext, boundary('word')))
# word = 단어 단위로 묶음
table(myword)
# https://www.rdocumentation.org/packages/searchable/versions/0.3.3.1/topics/boundary
table(tolower(myword))
# 일일이 설정 (고유명사 사전에 설정)
myword <- str_replace(myword, 'Trump', 'Trump_unique_')
myword <- str_replace(myword, 'States', 'States_unique_')
mytext <- c("He is one of statisticians agreeing that R is the No. 1 statistical software.","He is one of statisticians agreeing that R is the No. one statistical software.")
mytext
str_split(mytext, ' ')
# 삭제조건
# 숫자가 최소 1회 이상 연달아 등장 & 그 다음에는 공란이 1회 이상 등장
mytext2 <-str_split(str_replace_all(mytext, "[[:digit:]]{1,}[[:space:]]{1,}", ""), ' ')
mytext2
mytext2[[1]]
str_c(mytext2[[1]], collapse = ' ')
mytext <- "Baek et al. (2014) argued that the state of default-setting is critical for people to protect their own personal privacy on the Internet."
mytext
str_split(mytext, '\\. ')
str_split(mytext, ' ') # 공백문자로 구분
mytext <- c("She is an actor","She is the actor")
mytext
mystopwords <- 'a |an |the' #불용어 사전 정의
str_remove_all(mytext, mystopwords)
# install.packages('tm')
library(tm)
stopwords('en')
stopwords('SMART')
mytext
mystemmer.func <- function(mytextobj){
mytext<-str_replace_all(mytextobj, '(i|I)s |was |are |am |were ', 'be ')
mytext
}
mystemmer.func(mytext)
mytext <- c("I am a boy. You are a boy. The person might be a boy. Is Jane a boy?")
mystemmer.func(mytext)
mytext <- "The United States comprises fifty states. In the United States, each state has its own laws. However, federal law overrides state law in the United States."
mytext
myword<- unlist(str_extract_all(mytext, boundary('word')))
table(myword)
mytext.2gram <-str_replace_all(mytext, 'United States', 'United_States') #고유명사 처리
myword2<- unlist(str_extract_all(mytext.2gram, boundary('word')))
table(myword2)
length(table(myword2)) #단어 종류
sum(table(myword2)) #단어 전체 개수
mytext.3gram <-str_replace_all(mytext, '(t|T)he United States', 'United_States') #고유명사 처리
myword3<- unlist(str_extract_all(mytext.3gram, boundary('word')))
table(myword3)
length(table(myword3)) #단어 종류
sum(table(myword3)) #단어 전체 개수
my.text.location <- '../papers/papers'
mypaper<-VCorpus(DirSource(my.text.location))
mypaper #??분야 논문 코퍼스
summary(mypaper)
# mypaper 말붕치는 리스트 형식
mypaper[[2]]
mypaper[[2]]$content
mypaper[[2]]$meta
meta(mypaper[[2]], tag='author')<- 'Kim'
mypaper[[2]]$meta
myfunc <- function(x){
# print(x$content)
str_extract_all(x$content, "[[:alnum:]]{1,}[[:punct:]]{1}[[:alnum:]]{1,}")
}
mypuncts<-lapply(mypaper, myfunc)
table(unlist(mypuncts))
# 수치가 포함된 자료 추출
myfunc <- function(x){
str_extract_all(x$content, "[[:alpha:]]?[[:digit]]+[[:alpha:]]?")
}
mypuncts<-lapply(mypaper, myfunc)
table(unlist(mypuncts))
# tm_map함수
# tm.map(코펏, 처리작업)
res<-tm_map(mypaper, content_transformer(tolower))
res[[2]]$content
mypaper[[2]]$content
#숫자 제거
mycorpus <- tm_map(mypaper, removeNumbers)
mycorpus[[2]]$content
# 공백 2개 이상 -> 공백 1개
mycorpus <- tm_map(mycorpus, stripWhitespace)
mycorpus[[2]]$content
# 소문자로 변환
mycorpus <- tm_map(mycorpus, content_transformer(tolower))
mycorpus[[2]]$content
# 불용어 제거(mycorpus에서 stopwords('SMART') 모두 제거)
mycorpus <-tm_map(mycorpus, removeWords, words=stopwords('SMART'))
sms_raw<-read.csv('f:/data/sms_spam_ansi.txt')
str(sms_raw)
# 타겟변수는 범주형으로 되어야함
sms_raw$type <- factor(sms_raw$type)
table(sms_raw$type) # 데이터 빈도 확인
smscorpus<-VCorpus(VectorSource(sms_raw$text))
smscorpus
print(smscorpus)
inspect(smscorpus)
inspect(smscorpus[1:10])
#내용을 확인하려면 -> smscorpus[[1]]$content와 같음
as.character(smscorpus[[1]])
lapply(smscorpus[1:10], as.character)
#전처리 작업
smscorpus_clean <-tm_map(smscorpus, content_transformer(tolower))
lapply(smscorpus_clean[1:10], as.character)
#숫자 제거
smscorpus_clean<-tm_map(smscorpus_clean, removeNumbers)
#불용어 제거
smscorpus_clean<-tm_map(smscorpus_clean, removeWords, stopwords())
#특수문자 제거
smscorpus_clean<-tm_map(smscorpus_clean, removePunctuation)
#install.packages('SnowballC')
library(SnowballC)
wordStem(c('learn', 'learned', 'learning', 'learns'))
# 이것도 같음 (어근 추출출)
t<-c('learn', 'learned', 'learning', 'learns')
stemDocument(t)
smscorpus_clean<-tm_map(smscorpus_clean, stemDocument)
smscorpus_clean<-tm_map(smscorpus_clean, stripWhitespace)
lapply(smscorpus_clean[1:3], as.character)
lapply(smscorpus[1:3], as.character)
smsdtm<-DocumentTermMatrix(smscorpus_clean) #DTM
smsdtm
smsdtm_train<-smsdtm[1:4169,] #이메일 제목
smsdtm_test<-smsdtm[4170:5559,]
sms_train_labels<- sms_raw[1:4169,]$type
sms_test_labels<- sms_raw[4170:5559,]$type
prop.table(table(sms_train_labels))
prop.table(table(sms_test_labels))
# install.packages('wordcloud')
library(wordcloud)
wordcloud(smscorpus_clean, min.freq = 50) #최소 빈도수가 50이상
#어떤 단어가 많이 등장?
#스팸
#햄
sms_raw[sms_raw$type=='spam',]
spam<-subset(sms_raw, type='spam')
ham<-subset(sms_raw, type='ham')
wordcloud(spam$text, max.words=40)
wordcloud(ham$text, max.words=40)
smsdtm_freq_train<-removeSparseTerms(smsdtm_train, 0.999)
smsdtm_freq_train
findFreqTerms(smsdtm_train, 5)
smsfreqwords <- findFreqTerms(smsdtm_train, 5)
smsdtm_train[,smsfreqwords]
smsdtm_freq_train<-smsdtm_train[,smsfreqwords]
smsdtm_freq_test<-smsdtm_test[,smsfreqwords]
smsdtm_freq_train[[1]]
# smsdtm_freq_train[[1]]
converts_counts<-function(x){
x<-ifelse(x>0, 'Yes', 'No')
}
apply(smsdtm_train,2, converts_counts)
sms_train<-apply(smsdtm_train,2, converts_counts)
sms_test<-apply(smsdtm_test,2, converts_counts)
str(sms_train)
str(sms_test)
install.packages('e1071')
library(e1071)
sms_classifier <- naiveBayes(sms_train, sms_train_labels)
sms_classifier
sms_test_pred<-predict(sms_classifier, sms_test)
sms_test_pred
library(gmodels)
crossTable(sms_test_pred, sms_test_labels)
CrossTable(sms_test_pred, sms_test_labels)
install.packages('gpuR')
# install.packages('gpuR')
install.packages('devtools')
# install.packages('gpuR')
# install.packages('devtools')
library(gpuR)
# install.packages('gpuR')
# install.packages('devtools')
library(gpuR)
# install.packages('gpuR')
# install.packages('devtools')
library(gpuR)
# install.packages('gpuR')
# install.packages('devtools')
# library(gpuR)
library('gpuR')
mitcars
mtcars
midwest<-as.data.frame(ggplot2::midwest)
bananas <- c("banana", "Banana", "BANANA")
# .은 무엇이든 한 글자를 의미함
x <- c("apple", "banana", "pear")
str_extract(x, ".a.")
str_detect("\nX\n", ".X.") # 줄바꿈은 포함 안함
str_detect("\nX\n", regex(".X.", dotall = TRUE)) # dotall 은 .이 다음 줄에서도 영향
# grep (globally search a regular expression & print)
char1 <- c('apple','Apple','APPLE','banana','grape')
char1
grep('pp', char1)
grepl('pp', char1)
grep('pp', char1, value=T)
# grep()함수에 여러 패턴 사용하기
char2 <- c('apple','banana')
grep(char2,char1)  #패턴을 2개 이상 주면 첫 번째 패턴만 사용
paste(char2,collapse='|')
# regexpr(), gregexpr()
# 문자열에서 문자 위치 찾기
grep('-','010-8706-4712')  # grep으로는 위치를 찾을 수 없음.
regexpr('-','010-8706-4712')  # 처음 나오는 '-' 문자 위치 찾기
regexpr('-','010-8706-4712')  # 처음 나오는 '-' 문자 위치 찾기
# 나오는 '-' 문자 위치 모두 찾기 , g => global
gregexpr('-','010-8706-4712')
sub("p","*","apple") # substitute 대체하다
gsub("p","*","apple") # global
#글자크기, 각도 수정
ggplot(HR,aes(x = salary)) +
geom_bar(aes(fill = salary)) +
theme_bw() +
scale_fill_manual(values = c('red','royalblue','tan'))  +
coord_flip() +
theme(legend.position = 'none',
axis.text.x = element_text(size = 15,angle = 90),
axis.text.y = element_text(size = 15),
legend.text = element_text(size = 15))
library(ggplot2)
library(ggthemes)
#글자크기, 각도 수정
ggplot(HR,aes(x = salary)) +
geom_bar(aes(fill = salary)) +
theme_bw() +
scale_fill_manual(values = c('red','royalblue','tan'))  +
coord_flip() +
theme(legend.position = 'none',
axis.text.x = element_text(size = 15,angle = 90),
axis.text.y = element_text(size = 15),
legend.text = element_text(size = 15))
HR =read.csv('./HR_comma_sep.csv')
HR$left = as.factor(HR$left)
library(ggplot2)
library(ggthemes)
HR =read.csv('./HR_comma_sep.csv')
HR <-read.csv('./HR_comma_sep.csv')
# install.packages('ggthemes')
getwd
# install.packages('ggthemes')
getwd()
