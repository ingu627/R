str_split(mytext,' ') #결과는 list로 나옴
# 각 벡터의 길이(단어 개수) (결과 벡터로 출력)
sapply(str_split(mytext, ' '), length)
# 리스트로 출력
lapply(str_split(mytext, ' '), length)
# 문자 개수
sapply(str_split(mytext, ' '), str_length)
mytext
mytext.nowhitespace=str_replace_all(mytext, "[[:space:]]{1,}"," ")
sapply(str_split(mytext.nowhitespace, ' '), length)
sapply(str_split(mytext.nowhitespace, ' '), str_length)
# 대소문자 통일
mytext <- "The 45th President of the United States, Donald Trump, states that he knows how to play trump with the former president"
mytext
myword<-unlist(str_extract_all(mytext, boundary('word')))
# word = 단어 단위로 묶음
table(myword)
# https://www.rdocumentation.org/packages/searchable/versions/0.3.3.1/topics/boundary
table(tolower(myword))
# 일일이 설정 (고유명사 사전에 설정)
myword <- str_replace(myword, 'Trump', 'Trump_unique_')
myword <- str_replace(myword, 'States', 'States_unique_')
mytext <- c("He is one of statisticians agreeing that R is the No. 1 statistical software.","He is one of statisticians agreeing that R is the No. one statistical software.")
mytext
str_split(mytext, ' ')
# 삭제조건
# 숫자가 최소 1회 이상 연달아 등장 & 그 다음에는 공란이 1회 이상 등장
mytext2 <-str_split(str_replace_all(mytext, "[[:digit:]]{1,}[[:space:]]{1,}", ""), ' ')
mytext2
mytext2[[1]]
str_c(mytext2[[1]], collapse = ' ')
mytext <- "Baek et al. (2014) argued that the state of default-setting is critical for people to protect their own personal privacy on the Internet."
mytext
str_split(mytext, '\\. ')
str_split(mytext, ' ') # 공백문자로 구분
mytext <- c("She is an actor","She is the actor")
mytext
mystopwords <- 'a |an |the' #불용어 사전 정의
str_remove_all(mytext, mystopwords)
# install.packages('tm')
library(tm)
stopwords('en')
stopwords('SMART')
mytext
mystemmer.func <- function(mytextobj){
mytext<-str_replace_all(mytextobj, '(i|I)s |was |are |am |were ', 'be ')
mytext
}
mystemmer.func(mytext)
mytext <- c("I am a boy. You are a boy. The person might be a boy. Is Jane a boy?")
mystemmer.func(mytext)
mytext <- "The United States comprises fifty states. In the United States, each state has its own laws. However, federal law overrides state law in the United States."
mytext
myword<- unlist(str_extract_all(mytext, boundary('word')))
table(myword)
mytext.2gram <-str_replace_all(mytext, 'United States', 'United_States') #고유명사 처리
myword2<- unlist(str_extract_all(mytext.2gram, boundary('word')))
table(myword2)
length(table(myword2)) #단어 종류
sum(table(myword2)) #단어 전체 개수
mytext.3gram <-str_replace_all(mytext, '(t|T)he United States', 'United_States') #고유명사 처리
myword3<- unlist(str_extract_all(mytext.3gram, boundary('word')))
table(myword3)
length(table(myword3)) #단어 종류
sum(table(myword3)) #단어 전체 개수
my.text.location <- '../papers/papers'
mypaper<-VCorpus(DirSource(my.text.location))
mypaper #??분야 논문 코퍼스
summary(mypaper)
# mypaper 말붕치는 리스트 형식
mypaper[[2]]
mypaper[[2]]$content
mypaper[[2]]$meta
meta(mypaper[[2]], tag='author')<- 'Kim'
mypaper[[2]]$meta
myfunc <- function(x){
# print(x$content)
str_extract_all(x$content, "[[:alnum:]]{1,}[[:punct:]]{1}[[:alnum:]]{1,}")
}
mypuncts<-lapply(mypaper, myfunc)
table(unlist(mypuncts))
# 수치가 포함된 자료 추출
myfunc <- function(x){
str_extract_all(x$content, "[[:alpha:]]?[[:digit]]+[[:alpha:]]?")
}
mypuncts<-lapply(mypaper, myfunc)
table(unlist(mypuncts))
# tm_map함수
# tm.map(코펏, 처리작업)
res<-tm_map(mypaper, content_transformer(tolower))
res[[2]]$content
mypaper[[2]]$content
#숫자 제거
mycorpus <- tm_map(mypaper, removeNumbers)
mycorpus[[2]]$content
# 공백 2개 이상 -> 공백 1개
mycorpus <- tm_map(mycorpus, stripWhitespace)
mycorpus[[2]]$content
# 소문자로 변환
mycorpus <- tm_map(mycorpus, content_transformer(tolower))
mycorpus[[2]]$content
# 불용어 제거(mycorpus에서 stopwords('SMART') 모두 제거)
mycorpus <-tm_map(mycorpus, removeWords, words=stopwords('SMART'))
sms_raw<-read.csv('f:/data/sms_spam_ansi.txt')
str(sms_raw)
# 타겟변수는 범주형으로 되어야함
sms_raw$type <- factor(sms_raw$type)
table(sms_raw$type) # 데이터 빈도 확인
smscorpus<-VCorpus(VectorSource(sms_raw$text))
smscorpus
print(smscorpus)
inspect(smscorpus)
inspect(smscorpus[1:10])
#내용을 확인하려면 -> smscorpus[[1]]$content와 같음
as.character(smscorpus[[1]])
lapply(smscorpus[1:10], as.character)
#전처리 작업
smscorpus_clean <-tm_map(smscorpus, content_transformer(tolower))
lapply(smscorpus_clean[1:10], as.character)
#숫자 제거
smscorpus_clean<-tm_map(smscorpus_clean, removeNumbers)
#불용어 제거
smscorpus_clean<-tm_map(smscorpus_clean, removeWords, stopwords())
#특수문자 제거
smscorpus_clean<-tm_map(smscorpus_clean, removePunctuation)
#install.packages('SnowballC')
library(SnowballC)
wordStem(c('learn', 'learned', 'learning', 'learns'))
# 이것도 같음 (어근 추출출)
t<-c('learn', 'learned', 'learning', 'learns')
stemDocument(t)
smscorpus_clean<-tm_map(smscorpus_clean, stemDocument)
smscorpus_clean<-tm_map(smscorpus_clean, stripWhitespace)
lapply(smscorpus_clean[1:3], as.character)
lapply(smscorpus[1:3], as.character)
smsdtm<-DocumentTermMatrix(smscorpus_clean) #DTM
smsdtm
smsdtm_train<-smsdtm[1:4169,] #이메일 제목
smsdtm_test<-smsdtm[4170:5559,]
sms_train_labels<- sms_raw[1:4169,]$type
sms_test_labels<- sms_raw[4170:5559,]$type
prop.table(table(sms_train_labels))
prop.table(table(sms_test_labels))
# install.packages('wordcloud')
library(wordcloud)
wordcloud(smscorpus_clean, min.freq = 50) #최소 빈도수가 50이상
#어떤 단어가 많이 등장?
#스팸
#햄
sms_raw[sms_raw$type=='spam',]
spam<-subset(sms_raw, type='spam')
ham<-subset(sms_raw, type='ham')
wordcloud(spam$text, max.words=40)
wordcloud(ham$text, max.words=40)
smsdtm_freq_train<-removeSparseTerms(smsdtm_train, 0.999)
smsdtm_freq_train
findFreqTerms(smsdtm_train, 5)
smsfreqwords <- findFreqTerms(smsdtm_train, 5)
smsdtm_train[,smsfreqwords]
smsdtm_freq_train<-smsdtm_train[,smsfreqwords]
smsdtm_freq_test<-smsdtm_test[,smsfreqwords]
smsdtm_freq_train[[1]]
# smsdtm_freq_train[[1]]
converts_counts<-function(x){
x<-ifelse(x>0, 'Yes', 'No')
}
apply(smsdtm_train,2, converts_counts)
sms_train<-apply(smsdtm_train,2, converts_counts)
sms_test<-apply(smsdtm_test,2, converts_counts)
str(sms_train)
str(sms_test)
install.packages('e1071')
library(e1071)
sms_classifier <- naiveBayes(sms_train, sms_train_labels)
sms_classifier
sms_test_pred<-predict(sms_classifier, sms_test)
sms_test_pred
library(gmodels)
crossTable(sms_test_pred, sms_test_labels)
CrossTable(sms_test_pred, sms_test_labels)
install.packages('gpuR')
# install.packages('gpuR')
install.packages('devtools')
# install.packages('gpuR')
# install.packages('devtools')
library(gpuR)
# install.packages('gpuR')
# install.packages('devtools')
library(gpuR)
# install.packages('gpuR')
# install.packages('devtools')
library(gpuR)
# install.packages('gpuR')
# install.packages('devtools')
# library(gpuR)
library('gpuR')
mitcars
mtcars
midwest<-as.data.frame(ggplot2::midwest)
bananas <- c("banana", "Banana", "BANANA")
# .은 무엇이든 한 글자를 의미함
x <- c("apple", "banana", "pear")
str_extract(x, ".a.")
str_detect("\nX\n", ".X.") # 줄바꿈은 포함 안함
str_detect("\nX\n", regex(".X.", dotall = TRUE)) # dotall 은 .이 다음 줄에서도 영향
# grep (globally search a regular expression & print)
char1 <- c('apple','Apple','APPLE','banana','grape')
char1
grep('pp', char1)
grepl('pp', char1)
grep('pp', char1, value=T)
# grep()함수에 여러 패턴 사용하기
char2 <- c('apple','banana')
grep(char2,char1)  #패턴을 2개 이상 주면 첫 번째 패턴만 사용
paste(char2,collapse='|')
# regexpr(), gregexpr()
# 문자열에서 문자 위치 찾기
grep('-','010-8706-4712')  # grep으로는 위치를 찾을 수 없음.
regexpr('-','010-8706-4712')  # 처음 나오는 '-' 문자 위치 찾기
regexpr('-','010-8706-4712')  # 처음 나오는 '-' 문자 위치 찾기
# 나오는 '-' 문자 위치 모두 찾기 , g => global
gregexpr('-','010-8706-4712')
sub("p","*","apple") # substitute 대체하다
gsub("p","*","apple") # global
#글자크기, 각도 수정
ggplot(HR,aes(x = salary)) +
geom_bar(aes(fill = salary)) +
theme_bw() +
scale_fill_manual(values = c('red','royalblue','tan'))  +
coord_flip() +
theme(legend.position = 'none',
axis.text.x = element_text(size = 15,angle = 90),
axis.text.y = element_text(size = 15),
legend.text = element_text(size = 15))
library(ggplot2)
library(ggthemes)
#글자크기, 각도 수정
ggplot(HR,aes(x = salary)) +
geom_bar(aes(fill = salary)) +
theme_bw() +
scale_fill_manual(values = c('red','royalblue','tan'))  +
coord_flip() +
theme(legend.position = 'none',
axis.text.x = element_text(size = 15,angle = 90),
axis.text.y = element_text(size = 15),
legend.text = element_text(size = 15))
HR =read.csv('./HR_comma_sep.csv')
HR$left = as.factor(HR$left)
library(ggplot2)
library(ggthemes)
HR =read.csv('./HR_comma_sep.csv')
HR <-read.csv('./HR_comma_sep.csv')
# install.packages('ggthemes')
getwd
# install.packages('ggthemes')
getwd()
iris
df=iris
df
df=iris
df
str(df)
set.seed(1234)
sample(1:2, size=nrow(df), prob=c(0.7,0.3), replace=TRUE)
idex =sample(1:2, size=nrow(df), prob=c(0.7,0.3), replace=TRUE)
iris[idx==1,]
df[idx==1,]
idx =sample(1:2, size=nrow(df), prob=c(0.7,0.3), replace=TRUE)
df[idx==1,]
df[idx==2,]
df=iris
df
str(df)
idx =sample(1:nrow(df), size=nrow(df)*0.7 , replace=FALSE)
df[idx,]
train=df[idx,]
test = df[-idx,]
table(train$Species)
table(test$Species)
length(train$Species)
length(test$Species)
train
train_y= train$Species
test_y= test$Species
str(train)
str(test)
train = train[, -c(5)]
test = test[, -c(5)]
train = scale(train)
test = scale(test)
library(class)
iris_species = knn(train, test, cl=test_y, k=12)
train
iris_species = knn(train, test, cl=test_y, k=12)
iris_species = knn(train, test, cl=train_y, k=12)
iris_species
test_y
# Confusion Matrix 틀 만들기
result <- matrix(NA, nrow = 3, ncol = 3)
rownames(result) <- paste0("real_", levels(train_y))
colnames(result) <- paste0("clsf_", levels(train_y))
# Confusion Matrix 값 입력하기
result[1, 1] <- sum(ifelse(test_y == "setosa" & knn_21_test == "setosa", 1, 0))
result[2, 1] <- sum(ifelse(test_y == "versicolor" & knn_21_test == "setosa", 1, 0))
result[3, 1] <- sum(ifelse(test_y == "virginica" & knn_21_test == "setosa", 1, 0))
knn_21_test = knn(train, test, cl=train_y, k=21)
knn_21_test
test_y
# Confusion Matrix 틀 만들기
result <- matrix(NA, nrow = 3, ncol = 3)
rownames(result) <- paste0("real_", levels(train_y))
colnames(result) <- paste0("clsf_", levels(train_y))
# Confusion Matrix 값 입력하기
result[1, 1] <- sum(ifelse(test_y == "setosa" & knn_21_test == "setosa", 1, 0))
result[2, 1] <- sum(ifelse(test_y == "versicolor" & knn_21_test == "setosa", 1, 0))
result[3, 1] <- sum(ifelse(test_y == "virginica" & knn_21_test == "setosa", 1, 0))
result[1, 2] <- sum(ifelse(test_y == "setosa" & knn_21_test == "versicolor", 1, 0))
result[2, 2] <- sum(ifelse(test_y == "versicolor" & knn_21_test == "versicolor", 1, 0))
result[3, 2] <- sum(ifelse(test_y == "virginica" & knn_21_test == "versicolor", 1, 0))
result[1, 3] <- sum(ifelse(test_y == "setosa" & knn_21_test == "virginica", 1, 0))
result[2, 3] <- sum(ifelse(test_y == "versicolor" & knn_21_test == "virginica", 1, 0))
result[3, 3] <- sum(ifelse(test_y == "virginica" & knn_21_test == "virginica", 1, 0))
result
sum(knn_21_test == test_y) / sum(result)
df=iris
df
str(df)
set.seed(1234)
idx =sample(1:nrow(df), size=nrow(df)*0.7 , replace=FALSE)
train = df[idx,]
test = df[-idx,]
length(train$Species)
length(test$Species)
table(train$Species)
table(test$Species)
train_y= train$Species
test_y= test$Species
str(train)
str(test)
train = train[, -c(5)]
test = test[, -c(5)]
train = scale(train)
test = scale(test)
library(class)
knn_21_test = knn(train, test, cl=train_y, k=21)
knn_21_test
test_y
# Confusion Matrix 틀 만들기
result <- matrix(NA, nrow = 3, ncol = 3)
rownames(result) <- paste0("real_", levels(train_y))
colnames(result) <- paste0("clsf_", levels(train_y))
# Confusion Matrix 값 입력하기
result[1, 1] <- sum(ifelse(test_y == "setosa" & knn_21_test == "setosa", 1, 0))
result[2, 1] <- sum(ifelse(test_y == "versicolor" & knn_21_test == "setosa", 1, 0))
result[3, 1] <- sum(ifelse(test_y == "virginica" & knn_21_test == "setosa", 1, 0))
result[1, 2] <- sum(ifelse(test_y == "setosa" & knn_21_test == "versicolor", 1, 0))
result[2, 2] <- sum(ifelse(test_y == "versicolor" & knn_21_test == "versicolor", 1, 0))
result[3, 2] <- sum(ifelse(test_y == "virginica" & knn_21_test == "versicolor", 1, 0))
result[1, 3] <- sum(ifelse(test_y == "setosa" & knn_21_test == "virginica", 1, 0))
result[2, 3] <- sum(ifelse(test_y == "versicolor" & knn_21_test == "virginica", 1, 0))
result[3, 3] <- sum(ifelse(test_y == "virginica" & knn_21_test == "virginica", 1, 0))
result
sum(knn_21_test == test_y) / sum(result)
library(ggplot2)
library(dplyr)
STOCK = read.csv('./Uniqlo.csv')
STOCK$Date = as.Date(STOCK$Date)
STOCK = read.csv('./Uniqlo.csv')
read.csv('f:/data/groceries.csv')
install.packages('arules')
# install.packages('arules')
library(arules)
read.transactions('groceries.csv', sep = ',')
read.transactions('f:/data/groceries.csv', sep = ',')
summary(groceries)
groceries=read.transactions('f:/data/groceries.csv', sep = ',')
summary(groceries)
groceries
inspect(groceries)
inspect(groceries[1:10])
itemFrequency(groceries)
itemFrequency(groceries, type='absolute')
itemFrequency(groceries[,1:3])
groceries[,1:3]
itemFrequencyPlot(groceries)
itemFrequencyPlot(groceries, type='absolute')
#가장 많이 팔린 20개 상품 시각화
itemFrequencyPlot(groceries, topN=2, type='absolute')
#가장 많이 팔린 20개 상품 시각화
itemFrequencyPlot(groceries, topN=20, type='absolute')
#지지도가 0.1이상인 상품 시각화
itemFrequencyPlot(groceries, support=0.1)
matrix(c(1,1,1,0,0,
1,1,1,1,0,
0,0,1,1,0,
0,1,0,1,1,
0,0,0,1,0)
matrix(c(1,1,1,0,0,
1,1,1,1,0,
0,0,1,1,0,
0,1,0,1,1,
0,0,0,1,0))
matrix(c(1,1,1,0,0,
1,1,1,1,0,
0,0,1,1,0,
0,1,0,1,1,
0,0,0,1,0))
matrix(c(1,1,1,0,0,
1,1,1,1,0,
0,0,1,1,0,
0,1,0,1,1,
0,0,0,1,0), ncol=5)
matrix(c(1,1,1,0,0,
1,1,1,1,0,
0,0,1,1,0,
0,1,0,1,1,
0,0,0,1,0), ncol=5, byrow=TRUE)
mat=matrix(c(1,1,1,0,0,
1,1,1,1,0,
0,0,1,1,0,
0,1,0,1,1,
0,0,0,1,0), ncol=5, byrow=TRUE)
mat
paste('row', 1:5)
paste0('row', 1:%)
paste0('row', 1:5)
rownames(mat)<-paste0('row', 1:5)
mat
# colnames(mat)<-paste0()
letters
# colnames(mat)<-paste0()
letters[1:5]
colnames(mat)<- letters[1:5]
# letters[1:5]
mat
#row는 거래 데이터(장바구니)
#col는 아이템 이름
str(mat)
class(mat)
#행렬 데이터를 transactions 데이터로 변환
as(mat, 'transactions')
#행렬 데이터를 transactions 데이터로 변환
mat.trans<-as(mat, 'transactions')
mat.trans
summary(mat.trans)
mat
mat
inspect(mat.trans)
#데이터프레임 데이터를 transactions 데이터로 변환
as.data.frame(mat)
#데이터프레임 데이터를 transactions 데이터로 변환
df<- as.data.frame(mat)
df.trans<-as(df, 'transactions')
str(df.trans)
inspect(df.trans)
str(df)
#수치(numeric) -> logical
class(sapply(df, as.logical))
#수치(numeric) 데이터가 저장된 데이터프레임-> logical 데이터로 변환된 데이터프레임
# class(sapply(df, as.logical))
as.data.frame(sapply(df, as.logical))
#수치(numeric) 데이터가 저장된 데이터프레임-> logical 데이터로 변환된 데이터프레임
# class(sapply(df, as.logical))
df<- as.data.frame(sapply(df, as.logical))
df.trans<-as(df, 'transactions')
df.trans
summary(df.trans)
inspect(df.trans)
#list 데이터를 transactions 데이터로 변환
mylist <- list(row1=c("a","b","c"),
row2=c("a","d"),
row3=c("b","e"),
row4=c("a","d","e"),
row5=c("b","c","d"))
mylist
mylist.trans<-as(mylist, 'transactions')
summary(mylist.trans)
inspect(mylist.trans)
##################################################
#groceries 데이터 연관 규칙 생성
apriori(groceries, parameter = list(support=0.006, confidence-0.25))
##################################################
#groceries 데이터 연관 규칙 생성
apriori(groceries, parameter = list(support=0.006, confidence=0.25))
##################################################
#groceries 데이터 연관 규칙 생성
groceryrules<- apriori(groceries, parameter = list(support=0.006, confidence=0.25))
groceryrules
inspect(groceryrules[300:350])
inspect(groceryrules[1:10])
summary(groceryrules)
inspect(groceryrules[1:10])
sort(groceryrules)
sort(groceryrules, by='lift')
inspect(sort(groceryrules, by='lift')[1:10])
inspect(sort(groceryrules, by='lift')[1:30])
#관심있는 상품에 대한 연관규칙을 검색
# ex. beef와 함께 묶어서 판매할만한 상품?
beefrules<-subset(groceryrules, items %in% 'beef')
inspect(beefrules)
inspect(sort(beefrules, by='life')[1:5])
inspect(sort(beefrules, by='lift')[1:5])
write(beefrules, file='beefrules.csv', sep=',')
write(beefrules, file='beefrules.csv', sep=',')
#관심있는 상품에 대한 연관규칙을 검색
# ex. beef와 함께 묶어서 판매할만한 상품?
beefrules<-subset(groceryrules, items %in% 'beef')
beefrules
inspect(beefrules)
inspect(sort(beefrules, by='lift')[1:5])
write(beefrules, file='beefrules.csv', sep=',')
getwd()
